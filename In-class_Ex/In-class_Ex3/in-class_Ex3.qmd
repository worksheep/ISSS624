---
title: "Calibrating Spatial Interaction Models with R"
date: "02 December 2023"
date-modified: "last-modified"
format: html
execute:
  echo: true
  eval: true
  warning: false
editor: visual
---

## My own notes

### 3 basic types of interaction models

Pull factor & push factor can be different, 1 variable or multi-variable too

Gravity Model - Potential Model - how far are people away Retail Model - people who are closer to this trade area tend to go to this location, propensity that people will shop here instead of shop here base on the distance.

### Gravity Model

typical equation, equation decomposed to formula that can be addressed by regression formula. However not a typical multiple-linear regression. Distance are usually non-linear. Distance decay depends on the goods e.g. expensive goods, long longevity we will be ok to pay more, travel more to find the it. Whereas for a normal bread, we may not travel so far to get it.

IT people sometimes no choice, the industry is in Changi Business Park, then no choice we have to go there. Not every relationship is linear.

There are different type of calibration.

### Conceptually

we need 3 set of data a) flow data (accomplished in hands-on_Ex3) b) how to calculate Sij

## Overview

Spatial Interaction Models (SIMs) are mathematical models for estimating flows between spatial entities developed by Alan Wilson in the late 1960s and early 1970, with considerable uptake and refinement for transport modelling since then Boyce and Williams (2015).

There are four main types of traditional SIMs (Wilson 1971):

-   Unconstrained
-   Production-constrained
-   Attraction-constrained
-   Doubly-constrained

Ordinary least square (OLS), log-normal, Poisson and negative binomial (NB) regression methods have been used extensively to calibrate OD flow models by processing flow data as different types of dependent variables. In this chapter, you will gain hands-on experiences on using appropriate R packages to calibrate SIM by using there four regression methods.

::: callout-note
Calibration is the process of adjusting parameters in the model to try and get the estimates to agree with the observed data as much as possible. Adjusting the parameters is the sort of iterative process that computers are particularly good at and the goodness-of-fit statistics can be used to indicate when the optimum solution is found. Historically this process required a researcher with the requisite programming skills to write a computer algorithm to iteratively adjust each parameter, check the goodness-of-fit, and then start all over again until the goodness-of-fit statistic was maximised/minimised. (Adam Dennett, 2018)
:::

## The Case Study and Data

In this exercise, we are going to calibrate SIM to determine factors affecting the public bus passenger flows during the morning peak in Singapore.

## Getting Started

For the purpose of this exercise, four r packages will be used. They are:

-   sf for importing, integrating, processing and transforming geospatial data.
-   tidyverse for importing, integrating, wrangling and visualising data.
-   tmap for creating thematic maps.

```{r}
pacman::p_load(tmap, sf, sp, DT,
               performance, reshape2,
               ggpubr, tidyverse)
```

sp is a older package of sf. sp allow us to handle spatial object in spatial point, line and polygon dataframe. It is more efficient in computation of large data. therefore we do continue to use it from time to time when we are dealing with large data

performance is designed like tibble . easystats.github.io

reshape is the greatgrandfather of tidyr. pivot long, pivot fat. reshape has 1 interesting feature that allow handling of matrix. In tidyr, it is all in dataframe format, it doesn't handle matrix very well. Distance is matrix form, thus it is better to use reshape, despite it being "older". it serve very specifiy purpose, as we want to work with matrix

ggpubr create multiple plot

tidyverse compose of basic R packages to do data science work. Packages like dyplr, gglot2, forcats, tibble, readr, stringr, tidyr, purrr.

## The Data

This exercise is a continuation of **Hands-on_Ex3 : Processing and Visualising Flow Data** and the following data will be used:

-   *od_data.rds*, weekday morning peak passenger flows at planning subzone level.
-   *mpsz.rds*, URA Master Plan 2019 Planning Subzone boundary in simple feature tibble data frame format.

Beside these two data sets, an additional attribute data file called pop.csv will be provided. It

## Computing Distance Matrix

In spatial interaction, a distance matrix is a table that shows the distance between pairs of locations. For example, Euclidean distance of 3926.0025 between MESZ01 and RVSZ05, of 3939.1079 between MESZ01 and SRSZ01, and so on. By definition, an location's distance from itself, which is shown in the main diagonal of the table, is 0.

First, let us import *mpsz*

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                   layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)
mpsz
```

when we save into rds, it retain the r object, e.g. as simple feature (sf) tibble data.frame object class. However one downside is not all GIS software can read this.

### Converting from sf data.table to SpatialPolygonsDataFrame

There are at least two ways to compute the required distance matrix. One is based on sf and the other is based on sp. Past experience shown that computing distance matrix by using sf function took relatively longer time that sp method especially the data set is large. In view of this, sp method is used in the code chunks below.

First [`as.Spatial()`](https://r-spatial.github.io/sf/reference/coerce-methods.html) will be used to convert *mpsz* from sf tibble data frame to SpatialPolygonsDataFrame of sp object as shown in the code chunk below.

```{r}
mpsz_sp <- as(mpsz, "Spatial")
mpsz_sp
```

this is something that prof was going through on how to extract data from sp, but i missed that and couldn't exactly replicate the data

```         
mpsz_sp_selected <- mpsz_sp %>%
  select(mpsz_sp@data$SUBZONE_N)
  
```

```{r}
#this is usually how the prof write it
mpsz_sp_variant <- mpsz %>%
  as_Spatial()
```

### Computing the distance matrix

Next, [`spDists()`](https://www.rdocumentation.org/packages/sp/versions/2.1-1/topics/spDistsN1) of sp package will be used to compute the Euclidean distance between the centroids of the planning subzones.

::: callout-tip
### Q&A

Do you know why the distance is calculated between two centroids of a pair of spatial polygons?
:::

we have 332 subzone, so 332\^2 = 110224 given this is a distance matrix of each subzone to each subzone.

```{r}
dist <- spDists(mpsz_sp, 
                longlat = FALSE)

#this is just to print out a sample of 10 columns, 10 row
head(dist, n=c(10, 10))
```

Notice that the output *dist* is a matrix object class of R. Also notice that the column heanders and row headers are not labeled with the planning subzone codes.

### Labelling column and row heanders of a distance matrix

First, we will create a list sorted according to the the distance matrix by planning sub-zone code.

```{r}
#| eval: false
sz_names <- mpsz$SUBZONE_C
```

Next we will attach `SUBZONE_C` to row and column for distance matrix matching ahead

```{r}
#| eval: false
colnames(dist) <- paste0(sz_names)
rownames(dist) <- paste0(sz_names)
```

### Pivoting distance value by SUBZONE_C

Next, we will "unpivot" the distance matrix into a long table by using the row and column subzone codes as show in the code chunk below.

melt is a old reshape function.

```{r}
#| eval: false
distPair <- melt(dist) %>%
  rename(dist = value)
head(distPair, 10)
```

Notice that the within zone distance is 0.

### Updating intra-zonal distances

In this section, we are going to append a constant value to replace the intra-zonal distance of 0.

First, we will select and find out the minimum value of the distance by using `summary()`.

note: we didn't pass the result over a container, so it is only store in memory and does not change the existing feature

```{r}
#| eval: false
distPair %>%
  filter(dist > 0) %>%
  summary()
```

Next, a constant distance value of 50m is added into intra-zones distance. The smallest inter-zonal distance is 173.8m, so we want intra-zonal to be smaller. since cross zone from centroid to centroid is 173.8m, then the distance from intra-zonal from centroid to border will be \<80+m, which we simplify to 50m

another way is to calculate the perimeter, then find the shortest distance from centroid to the border for each subzone.

```{r}
#| eval: false
distPair$dist <- ifelse(distPair$dist == 0,
                        50, distPair$dist)
```

The code chunk below will be used to check the result data.frame.

```{r}
#| eval: false
distPair %>%
  summary()
```

The code chunk below is used to rename the origin and destination fields.

```{r}
#| eval: false
distPair <- distPair %>%
  rename(orig = Var1,
         dest = Var2)
```

Lastly, the code chunk below is used to save the dataframe for future use.

```{r}
#| eval: false
write_rds(distPair, "data/rds/distPair.rds") 
```

## Preparing flow data

The code chunk below is used import *od_data* save in Chapter 15 into R environment.

```{r}
#| eval: false
od_data <- read_rds("data/rds/od_data.rds")
```

Next, we will compute the total passenger trip between and within planning subzones by using the code chunk below. The output is all *flow_data*.

```{r}
#| eval: false
flow_data <- od_data %>%
  group_by(ORIGIN_SZ, DESTIN_SZ) %>% 
  summarize(TRIPS = sum(MORNING_PEAK)) 
```

Use the code chunk below to display flow_data dataframe.

```{r}
#| eval: false
head(flow_data, 10)
```

### Separating intra-flow from passenger volume df

Code chunk below is used to add three new fields in `flow_data` dataframe.

```{r}
#| eval: false
flow_data$FlowNoIntra <- ifelse(
  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, 
  0, flow_data$TRIPS)
flow_data$offset <- ifelse(
  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, 
  0.000001, 1)
```

### Combining passenger volume data with distance value

Before we can join *flow_data* and *distPair*, we need to convert data value type of *ORIGIN_SZ* and *DESTIN_SZ* fields of flow_data dataframe into factor data type.

```{r}
#| eval: false
flow_data$ORIGIN_SZ <- as.factor(flow_data$ORIGIN_SZ)
flow_data$DESTIN_SZ <- as.factor(flow_data$DESTIN_SZ)
```

Now, `left_join()` of **dplyr** will be used to *flow_data* dataframe and *distPair* dataframe. The output is called *flow_data1*.

```{r}
#| eval: false
flow_data1 <- flow_data %>%
  left_join (distPair,
             by = c("ORIGIN_SZ" = "orig",
                    "DESTIN_SZ" = "dest"))
```

## Preparing Origin and Destination Attributes

### Importing population data

the data is [extracted from Singstat from Department of Stats](https://www.singstat.gov.sg/find-data/search-by-theme/population/geographic-distribution/latest-data)

```{r}
#| eval: false
pop <- read_csv("data/aspatial/pop.csv")
```

### Geospatial data wrangling

```{r}
#| eval: false
pop <- pop %>%
  left_join(mpsz,
            by = c("PA" = "PLN_AREA_N",
                   "SZ" = "SUBZONE_N")) %>%
  select(1:6) %>%
  rename(SZ_NAME = SZ,
         SZ = SUBZONE_C)
```

### Preparing origin attribute

```{r}
#| eval: false
flow_data1 <- flow_data1 %>%
  left_join(pop,
            by = c(ORIGIN_SZ = "SZ")) %>%
  rename(ORIGIN_AGE7_12 = AGE7_12,
         ORIGIN_AGE13_24 = AGE13_24,
         ORIGIN_AGE25_64 = AGE25_64) %>%
  select(-c(PA, SZ_NAME))

```

### Preparing destination attribute

```{r}
#| eval: false
flow_data1 <- flow_data1 %>%
  left_join(pop,
            by = c(DESTIN_SZ = "SZ")) %>%
  rename(DESTIN_AGE7_12 = AGE7_12,
         DESTIN_AGE13_24 = AGE13_24,
         DESTIN_AGE25_64 = AGE25_64) %>%
  select(-c(PA, SZ_NAME))
```

We will called the output data file *SIM_data*. it is in rds data file format.

```{r}
#| eval: false
write_rds(flow_data1, "data/rds/SIM_data")
```

## Calibrating Spatial Interaction Models

In this section, you will learn how to calibrate Spatial Interaction Models by using Poisson Regression method.

### Importing the modelling data

Firstly, let us import the modelling data by using the code chunk below.

```{r}
SIM_data <- read_rds("data/rds/SIM_data.rds")
```

### Visualising the dependent variable

Firstly, let us plot the distribution of the dependent variable (i.e. TRIPS) by using histogram method by using the code chunk below.

```{r}
ggplot(data = SIM_data,
       aes(x = TRIPS)) +
  geom_histogram()
```

Notice that the distribution is highly skewed and not resemble bell shape or also known as normal distribution.

Next, let us visualise the relation between the dependent variable and one of the key independent variable in Spatial Interaction Model, namely distance.

```{r}
ggplot(data = SIM_data,
       aes(x = dist,
           y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)
```

Notice that their relationship hardly resemble linear relationship.

On the other hand, if we plot the scatter plot by using the log transformed version of both variables, we can see that their relationship is more resemble linear relationship.

```{r}
ggplot(data = SIM_data,
       aes(x = log(dist),
           y = log(TRIPS))) +
  geom_point() +
  geom_smooth(method = lm)
```

### Checking for variables with zero values

Since Poisson Regression is based of log and log 0 is undefined, it is important for us to ensure that no 0 values in the explanatory variables.

In the code chunk below, summary() of Base R is used to compute the summary statistics of all variables in *SIM_data* data frame.

```{r}
summary(SIM_data)
```

The print report above reveals that variables ORIGIN_AGE7_12, ORIGIN_AGE13_24, ORIGIN_AGE25_64,DESTIN_AGE7_12, DESTIN_AGE13_24, DESTIN_AGE25_64 consist of 0 values.

In view of this, code chunk below will be used to replace zero values to 0.99.

we don't do this upfront because the data should be clean earlier. For this modelling, that we want to make it fit into a model that this is feature engineering.

```{r}
SIM_data$DESTIN_AGE7_12 <- ifelse(
  SIM_data$DESTIN_AGE7_12 == 0,
  0.99, SIM_data$DESTIN_AGE7_12)
SIM_data$DESTIN_AGE13_24 <- ifelse(
  SIM_data$DESTIN_AGE13_24 == 0,
  0.99, SIM_data$DESTIN_AGE13_24)
SIM_data$DESTIN_AGE25_64 <- ifelse(
  SIM_data$DESTIN_AGE25_64 == 0,
  0.99, SIM_data$DESTIN_AGE25_64)
SIM_data$ORIGIN_AGE7_12 <- ifelse(
  SIM_data$ORIGIN_AGE7_12 == 0,
  0.99, SIM_data$ORIGIN_AGE7_12)
SIM_data$ORIGIN_AGE13_24 <- ifelse(
  SIM_data$ORIGIN_AGE13_24 == 0,
  0.99, SIM_data$ORIGIN_AGE13_24)
SIM_data$ORIGIN_AGE25_64 <- ifelse(
  SIM_data$ORIGIN_AGE25_64 == 0,
  0.99, SIM_data$ORIGIN_AGE25_64)

```

You can run the summary() again.

```{r}
summary(SIM_data)
```

Notice that all the 0 values have been replaced by 0.99.

### Unconstrained Spatial Interaction Model

In this section, you will learn how to calibrate an unconstrained spatial interaction model by using `glm()` of Base Stats. The explanatory variables are origin population by different age cohort, destination population by different age cohort (i.e. *ORIGIN_AGE25_64*) and distance between origin and destination in km (i.e. *dist*).

The general formula of Unconstrained Spatial Interaction Model

![](chap15/img/image1.jpg){width="480"}

The code chunk used to calibrate to model is shown below:

glm has many regression that can be used, not just least square. so we need to specify the family and link

the distance base on the formula is inverse. closer is what you preferred. the parameter should be negative, but the variable itself, we just add as positive.

```{r}
uncSIM <- glm(formula = TRIPS ~ 
                log(ORIGIN_AGE25_64) + 
                log(DESTIN_AGE25_64) +
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
uncSIM
```

### R-squared function

In order to measure how much variation of the trips can be accounted by the model we will write a function to calculate R-Squared value as shown below.

```{r}
CalcRSquared <- function(observed,estimated){
  r <- cor(observed,estimated)
  R2 <- r^2
  R2
}
```

Next, we will compute the R-squared of the unconstrained SIM by using the code chunk below.

```{r}
CalcRSquared(uncSIM$data$TRIPS, uncSIM$fitted.values)
```

```{r}
r2_mcfadden(uncSIM)
```

### Origin (Production) constrained SIM

In this section, we will fit an origin constrained SIM by using the code3 chunk below.

The general formula of Origin Constrained Spatial Interaction Model

![](chap15/img/image2.jpg){width="480"}

```{r}
orcSIM <- glm(formula = TRIPS ~ 
                 ORIGIN_SZ +
                 log(DESTIN_AGE25_64) +
                 log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
summary(orcSIM)
```

We can examine how the constraints hold for destinations this time.

```{r}
CalcRSquared(orcSIM$data$TRIPS, orcSIM$fitted.values)
```

### Destination constrained

In this section, we will fit a destination constrained SIM by using the code chunk below.

The general formula of Destination Constrained Spatial Interaction Model

![](chap15/img/image3.jpg){width="480"}

```{r}
decSIM <- glm(formula = TRIPS ~ 
                DESTIN_SZ + 
                log(ORIGIN_AGE25_64) + 
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
summary(decSIM)
```

We can examine how the constraints hold for destinations this time.

```{r}
CalcRSquared(decSIM$data$TRIPS, decSIM$fitted.values)
```

### Doubly constrained

In this section, we will fit a doubly constrained SIM by using the code chunk below.

The general formula of Doubly Constrained Spatial Interaction Model

![](chap15/img/image4.jpg){width="480"}

```{r}
dbcSIM <- glm(formula = TRIPS ~ 
                ORIGIN_SZ + 
                DESTIN_SZ + 
                log(dist),
              family = poisson(link = "log"),
              data = SIM_data,
              na.action = na.exclude)
summary(dbcSIM)
```

We can examine how the constraints hold for destinations this time.

```{r}
CalcRSquared(dbcSIM$data$TRIPS, dbcSIM$fitted.values)
```

Notice that there is a relatively greater improvement in the R\^2 value.

### Model comparison

Another useful model performance measure for continuous dependent variable is [Root Mean Squared Error](https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e). In this sub-section, you will learn how to use [`compare_performance()`](https://easystats.github.io/performance/reference/compare_performance.html) of [**performance**](https://easystats.github.io/performance/index.html) package

First of all, let us create a list called *model_list* by using the code chun below.

```{r}
model_list <- list(unconstrained=uncSIM,
                   originConstrained=orcSIM,
                   destinationConstrained=decSIM,
                   doublyConstrained=dbcSIM)
```

Next, we will compute the RMSE of all the models in *model_list* file by using the code chunk below.

```{r}
compare_performance(model_list,
                    metrics = "RMSE")
```

The print above reveals that doubly constrained SIM is the best model among all the four SIMs because it has the smallest RMSE value of 1487.111.

### Visualising fitted

In this section, you will learn how to visualise the observed values and the fitted values.

Firstly we will extract the fitted values from each model by using the code chunk below.

```{r}
df <- as.data.frame(uncSIM$fitted.values) %>%
  round(digits = 0)
```

Next, we will join the values to *SIM_data* data frame.

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(uncTRIPS = "uncSIM$fitted.values")
```

Repeat the same step by for Origin Constrained SIM (i.e. orcSIM)

```{r}
df <- as.data.frame(orcSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(orcTRIPS = "orcSIM$fitted.values")
```

Repeat the same step by for Destination Constrained SIM (i.e. decSIM)

```{r}
df <- as.data.frame(decSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(decTRIPS = "decSIM$fitted.values")
```

Repeat the same step by for Doubly Constrained SIM (i.e. dbcSIM)

```{r}
df <- as.data.frame(dbcSIM$fitted.values) %>%
  round(digits = 0)
```

```{r}
SIM_data <- SIM_data %>%
  cbind(df) %>%
  rename(dbcTRIPS = "dbcSIM$fitted.values")
```

```{r}
#| fig-height: 8
unc_p <- ggplot(data = SIM_data,
                aes(x = uncTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

orc_p <- ggplot(data = SIM_data,
                aes(x = orcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

dec_p <- ggplot(data = SIM_data,
                aes(x = decTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

dbc_p <- ggplot(data = SIM_data,
                aes(x = dbcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

ggarrange(unc_p, orc_p, dec_p, dbc_p,
          ncol = 2,
          nrow = 2)

```
