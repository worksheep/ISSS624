---
title: "In Class Ex4"
date: "09 December 2023"
date-modified: "last-modified"
format: html
execute:
  echo: true
  eval: true
  warning: false
editor: visual
---

## My own notes

after looking at the Hand-on Ex3, the pattern is displayed, but what are the factors contributing to this differences. Can we build the model. Spatial Interaction Model. Professional Geographer, built model to explain this phenomenon.

If you move the pivot to the lighter side in a see-saw, it will help to balance. Similarly, in a trip flow, between pushing & pulling, the gravitational model make use of similar idea. Calibrating the model in statistics, is usually multiple linear regression. In this topic, we are specifically using possion regression. We don't have negative trip, in terms of trips who should be in whole number, instead of decimal and this is not in line with the linear regression, thus we use possion regression.

GLM instead of LM, generalised linear regression model, that is usually used during machine learning.

School Directory and Information, in the morning especially secondary school & JC student. Poly, Uni student have varying time schedule

80% of the population stay in HDB, so we can use HDB property information from data.gov.sg to plot it into the hexagons to proxy for the population. \# of dwelling unit is a better option than \# blocks, because block can have different \# units, In addition, we can use the distribution of 1room, 2room etc to 5room and national average to can a "sumproduct" to proxy for population by block

Business - small scale, large scale, alot of them at the industrial park. Prof has a way to do this every quarter.

Fin Services - in Singapore, we have quite a lot of regional headquarter, and they are concentrated. The money changer, are not a store but a building.

This is to be used as factor, journey to work, weekend journey. No need to use all, shop usually open at 10am, except F&B for breakfast.

For lesisure, re-creational for example we can look at theatre only if we want to focus

12 years ago, Prof prepared a dataset for crime that is extrapolated from other places but proxy to Singapore, the students went to use it for other classes thinking it is the truth\* and the other prof shared with MHA that led to MHA calling up Prof.

## Getting Started

```{r}
pacman:: p_load(tidyverse, sf, httr, tmap)
```

HTTR is a r package similarly like python, that allow us to work with html pages

if we only want a specific location, LATLONG is the most convinient, it is align with Satellite. Total 24 of them, tri-angulated by 3 satellite.

1 degree distance at the equator is not the same as 1 degree distance at the north pole. So if we want to measure distance or area, we should go for projected coordinate system. We use the great circle method.

For onemap if we

## Geocoding using SLA OneMap API

Address geocoding, or simply geocoding, is the process of taking

```{r}
#| eval: false

url <- "https://www.onemap.gov.sg/api/common/elastic/search"

csv <- read_csv("data/aspatial/Generalinformationofschools.csv")

#this is saving the postal_code separately as a list
postcodes <- csv$`postal_code`


#initialisation, empty dataframe
found <- data.frame()
not_found <- data.frame()


for (postcode in postcodes){
  query <- list('searchVal'=postcode, 'returnGeom' = 'Y', 'getAddrDetails' = 'Y', 'pageNum' = '1')
  res <- GET(url, query=query)
  
  if((content(res)$found) != 0){
    found <- rbind(found,data.frame(content(res))[4:13])
  } else {
    not_found = data.frame(postcode)
  }
}
```

note : 1) #\| eval:false is for 2) message false 3) echo the code chunk will not show

the API call is new. so the old search is not the same

```
{content(res)}
```

```
merged = merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE)
write.csv(merged, file = 'data/aspatial/schools.csv')
write.csv(not_found, file = "data/aspatial/not_found.csv")
```

There's decimal degree and there's also minute second degree.

```{r}
newschoolread <- read_csv("data/aspatial/schools_updated.csv") %>%
  rename(longtitude = results.LONGITUDE,
         latitude = results.LATITUDE) %>%
  select(postal_code, school_name, longtitude, latitude)
```

```{r}
schools_sf <- st_as_sf(newschoolread,
                       coords = c("longtitude","latitude"),
                       crs = 4326
                       ) %>%
  st_transform(crs = 3414) %>%
  st_make_valid()
```

we put as crs = 4326 first because it was in WGS84 as longtitude latitude (degree). Thereafter, then we convert it to SVY21 under crs = 3414

```{r}
tmap_mode("view")
tm_shape(schools_sf %>%
           select(school_name,everything()))+
  tm_dots(col="black", 
              alpha = 0.6,
              border.alpha = 0.4) + 
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

```{r}

mpsz <- st_read(dsn = "data/geospatial", layer = "MPSZ-2019") %>% st_transform(crs = 3414)

#Merge
sgp <- st_union(mpsz$geometry)

# Simplify the geometry
sgp_simple <- st_simplify(sgp, dTolerance = 100) %>% 
  st_as_sf() %>%
  st_transform(crs = 3414) %>%
  st_make_valid()
  
tm_shape(sgp_simple)+
  tm_polygons(col="white", 
              alpha = 0.6,
              border.alpha = 0.4)
```

For coding, we need to make sure the coordinate system is the same, for ArcGIS / QGIS we have projection on the fly, so it can work

```{r}
mpsz$`SCHOOL_COUNT` <- lengths(st_intersects(mpsz, schools_sf))
tm_shape(mpsz)+
  tm_polygons(col="white", 
              alpha = 0.6,
              border.alpha = 0.4) +
  tm_shape(schools_sf)+
  tm_dots()
```

it is a generic application of length to get the "count"

```{r}
business <- st_read(dsn = "data/geospatial", layer = "Business") %>% st_transform(crs = 3414)

tm_shape(sgp_simple)+
  tm_polygons(col="white", 
              alpha = 0.6,
              border.alpha = 0.4) +
  tm_shape(business)+
  tm_dots()
```

this is the business SME (small medium enterprises)

```{r}
mpsz$`BUSINESS_COUNT` <- lengths(st_intersects(mpsz, business))
tm_shape(mpsz)+
  tm_polygons(col="white", 
              alpha = 0.6,
              border.alpha = 0.4) +
  tm_shape(business)+
  tm_dots()
```

```{r}
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz)+
  tm_polygons(col="white", 
              alpha = 0.6,
              border.alpha = 0.4) +
  tm_shape(business)+
  tm_dots()
```

the trick is to always plot the polygon first then the dots. tmap_options(check.and.fix) is to allow ploting of not-closed polygons

```         
flow_data <- flow_data %>%
  left_join(mpsz_tidy,
            by = c("DESTIN_SZ" = "SUBZONE_C"))
```

the attractiveness is referring to the destination. take note of that because all these variables are representing the attractiveness at the destination. If we are looking at attractiveness of the origin, we need to switch it. Calibrating journey to home, it becomes origin. Where are we modelling. Journey to home after work, the business are the "pushing" factor from the origin instead of the "pulling" factor of the destination.

```         
summary(flow_data)
```

before we do the log function, we need to check the data, make sure there is no 0, log 0 is error. So we assign a small value to make sure it is not 0, as long as not 1, because there could be count with 1

```         
flow_data$SCHOOL_COUNT <- ifelse(flow_data$SCHOOL_COUNT == 0, 0.99, flow_data$SCHOOL_COUNT)
```

### Going through the Take-Home 2

there was a study by a NTU group that through the survey that the maximum willingness to walk is about 750m. Therefore we use 325m

Choose only 1 time period. Each of them have meaning.

Those are limitation, by right we want to know the day. Friday, Sat, we might chill out, but Sunday we getting ready for monday blue, so we might not go out.

The attractiveness is different.

## **PART B**

```{r}
pacman::p_load(tmap, sf, performance, ggpubr, tidyverse)
```

performance is to compuete the performance matrix, to compute MSE (Mean Square Error) ggpubr is to gel several plot into 1

```{r}
flow_data <- read_rds("data/rds/flow_data_tidy.rds")
glimpse(flow_data)
```

R is case senstitive, MORNING_PEAK will be renamed back to TRIPS, then dist to DIST to prevent confusion with R's function dist

```{r}
flow_data$FlowNoIntra <- ifelse(
  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ,
  0, flow_data$MORNING_PEAK)

flow_data$offset <- ifelse(
  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ,
  0.000001,1
)

inter_zonal_flow <- flow_data %>%
  filter(FlowNoIntra > 0)

inter_zonal_flow <- inter_zonal_flow %>%
  rename(TRIPS = MORNING_PEAK,
         DIST = dist)

```

try to save the data, then we can add the line eval = false, to straight read the data. Then when we move from data cleaning to data modelling, it is good to clear the whole environment.

```{r}
options(max.print = 99999)

```

```{r}
orcSIM_Poisson <- glm(formula = TRIPS ~
                     ORIGIN_SZ +
                     log(SCHOOL_COUNT) +
                     log(RETAIL_COUNT) +
                     log(DIST) -1,
                     family = poisson(link = "log"),
                     data = inter_zonal_flow,
                     na.action = na.exclude
                     )
summary(orcSIM_Poisson, maxsum=nlevels(orcSIM_Poisson))
```

the purpose of the -1 is to remove the intercept. For the origin, destination constraint, it is already constrained, so there is no such thing as intercept. This is different from the in-class Ex3, while

we already control the origin / destination, there is no such thing as intercept. The idea of intercept is to control the overflow.

we need to look at the parameter, alpha, gamma, beta

for distance, we should look at inverse relationship. log(DIST) should be -ve. T

attractiveness should always be +ve. In Singapore, it is unusual, but in other places, it could be e.g. crime rate, you don't want to go shopping / reside in a place with high crime rate

```{r}
CalcRSquared <- function(observed,estimated){
  r <- cor(observed,estimated)
  R2 <- r^2
  R2
}
```

```{r}
CalcRSquared(orcSIM_Poisson$data$TRIPS, orcSIM_Poisson$fitted.values)
```

notice that we use data from specific field, in R datamodel, we have the trips in a list. This is a list of 30 objects. The list contain multiple information.

if we want to convert to dataframe, we just to extract, for example

```{r}
someerrorextract <- orcSIM_Poisson$residuals %>%
  as.data.frame()
```

but when we extract, we don't know which it correlates to becuase it don't have the name, so we need to extract the name, and join it back as per in-class_Ex3

the fitted value is the estimated value.

```{r}
performance_rmse(orcSIM_Poisson,
                 normalized = FALSE)
```

normalised = TRUE, it will standardise the value. mean = 0, std = 1, if we set it as false then it is a raw value

Doubly constrainted

```{r}

dbcSIM_Poisson <- glm(formula = TRIPS ~
                        ORIGIN_SZ + 
                        DESTIN_SZ +
                        log(DIST),
                      family = poisson(link = "log"),
                      data = inter_zonal_flow,
                      na.action = na.exclude)

summary(dbcSIM_Poisson)
```

doubly constraint no attractive variable.

```
model_list <- list(unconstrained = uncSIM,
                   originConstrained = orcSIM,
                   desintationConstrained = decSIM,
                   doublyConstrained = dbcSIM)
```

```
compare_performance(model_list,
                    metrics = "RMSE")
```

the smallest RMSE is the best.

in the in-class exercise 3, we can also visually see the RMSE, the bigger the RMSE, the data point are not as a close fit

then there's a big outlier, we might want to find out the big outlier. the model skew it? if we remove the outlier, how does that affect the model.

the outlier shift the line? will the model perform better
